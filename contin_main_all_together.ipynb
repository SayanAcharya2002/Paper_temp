{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random,os,time\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,f1_score,classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import itertools as it\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpath='https://raw.githubusercontent.com/ahmed-shameem/Class_imbalance/master/CI_Datasets/abalone.csv'\n",
    "file_name=urlpath.split('/')[-1]\n",
    "if not os.path.exists('file_name'):\n",
    "  os.system(f\"curl --url {urlpath} -o {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=[\n",
    "  # 'yeast5.csv',\n",
    "  # 'segment0.csv',\n",
    "  # 'abalone19.csv',\n",
    "  # 'abalone9-18.csv',\n",
    "  # 'cancer_classification.csv',\n",
    "  # 'hayes-roth.csv',\n",
    "  # 'page-blocks_csv.csv',\n",
    "  # 'kddcup-guess_passwd_vs_satan.csv',\n",
    "  # 'led7digit-0-2-4-5-6-7-8-9_vs_1.csv',\n",
    "  # 'new-thyroid.csv',\n",
    "  'new-thyroid.csv','abalone19.csv','led7digit-0-2-4-5-6-7-8-9_vs_1.csv','abalone.csv',\n",
    " 'abalone9-18.csv',\n",
    " 'cancer_classification.csv',\n",
    " 'hayes-roth.csv',\n",
    " 'kddcup-guess_passwd_vs_satan.csv',\n",
    " 'page-blocks_csv.csv',\n",
    "]\n",
    "\n",
    "f1_files=[\n",
    "  'abalone19.csv',\n",
    "  'SPECTF.csv',\n",
    "  'bupa.csv',\n",
    "  'new-thyroid.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abalone19.csv', 'SPECTF.csv', 'bupa.csv', 'new-thyroid.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_files=list(i for i in os.listdir() if i.endswith('.csv') and i not in exclude and i not in f1_files)\n",
    "all_files=f1_files\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28032/564858012.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_train_x_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst_train_y_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPSO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m     \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_percent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_inc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0mtotal_majority\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mmax_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28032/564858012.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, select_percent, p_inc, bounds)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m           \u001b[1;31m#temp_population=self.sigmoid_transform(temp_population)#making the floats 0/1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m           \u001b[0mtemp_fitness\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompound_obj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_population\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# calling the compound function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m           \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_fitness\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mbest_fitness_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_population\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28032/564858012.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    204\u001b[0m       \u001b[1;31m# [0,2,0]*[1,1,1]=[0,2,0]=>2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m       \u001b[1;31m# x=>agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[0mcompound_obj_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpenalty_constant\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthresolding_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28032/564858012.py\u001b[0m in \u001b[0;36mfitness\u001b[1;34m(self, agent, thresold)\u001b[0m\n\u001b[0;32m     79\u001b[0m       \u001b[1;31m# model=clf.fit(train_data,trainy[rows])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m       \u001b[0mabc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m       \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# Boosting step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \"\"\"\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"SAMME.R\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# raise Exception(\"Fix the directory of result+new_output.csv\")\n",
    "\n",
    "for file_name in all_files:\n",
    "  df=pd.read_csv(file_name)\n",
    "  df\n",
    "  df.columns=map(lambda x:x.strip(),df.columns)\n",
    "  # df.iloc[:,-1].value_counts()\n",
    "  if file_name==\"hayes-roth.csv\":\n",
    "    df[df.iloc[:,-1]==1]=0\n",
    "    df[df.iloc[:,-1]==2]=0\n",
    "    df[df.iloc[:,-1]==3]=1\n",
    "  elif file_name=='page-blocks_csv.csv':\n",
    "    maj_index=[1,2,3,4]\n",
    "    min_index=[5]\n",
    "    for j in maj_index:\n",
    "      df[df.iloc[:,-1]==j]=1\n",
    "    for j in min_index:\n",
    "      df[df.iloc[:,-1]==j]=0\n",
    "\n",
    "  min_index=1 if len(df[df.iloc[:,-1]==1])<len(df[df.iloc[:,-1]==0]) else 0\n",
    "  max_index=1-min_index\n",
    "  X=df.iloc[:,:-1]\n",
    "  y=df.iloc[:,-1]\n",
    "  first_train_x,final_test_x,first_train_y,final_test_y=train_test_split(X,y,test_size=0.2,random_state=int(time.time()))\n",
    "\n",
    "  first_train_x_array=np.array(first_train_x)\n",
    "  first_train_y_array=np.array(first_train_y)\n",
    "\n",
    "\n",
    "  def obj_func(population:np.array,model=AdaBoostClassifier(n_estimators=50,learning_rate=1)):\n",
    "    indices=[]\n",
    "    for i in range(population):\n",
    "      if population[i]==1:\n",
    "        indices.append(i)\n",
    "    total_data=df.iloc[indices,:]\n",
    "    train_x,test_x,train_y,test_y=train_test_split(total_data.iloc[:,:-1],total_data.iloc[:,-1],test_size=0.2,random_state=int(time.time()))\n",
    "    model.fit(train_x,train_y)\n",
    "    return model.score(test_x,test_y)\n",
    "  #### PSO\n",
    "  class PSO:\n",
    "    def __init__(self,agent_num,max_iter,obj_func,train_x,test_x,train_y,test_y,method_name=\"roc_auc\"):\n",
    "      \n",
    "      self.method_name=method_name.lower()\n",
    "      self.loggings=[]\n",
    "      self.agent_num=agent_num\n",
    "      self.max_iter=max_iter\n",
    "      \n",
    "      # self.obj_func=obj_func\n",
    "      self.obj_func=self.fitness\n",
    "\n",
    "      self.train_x=train_x.copy()\n",
    "      self.test_x=test_x.copy()\n",
    "      self.train_y=train_y.copy()\n",
    "      self.test_y=test_y.copy()\n",
    "\n",
    "      self.majority_index=None\n",
    "      self.minority_index=None\n",
    "\n",
    "      self.worst_cases=None\n",
    "\n",
    "      self.p_inc=1\n",
    "\n",
    "      \n",
    "\n",
    "    def fitness(self,agent,thresold=0.5):\n",
    "      # clf=KNeighborsClassifier(n_neighbors=5)\n",
    "      rows1 = []\n",
    "      for i in range(len(agent)):\n",
    "        if(agent[i]>thresold):\n",
    "          rows1.append(self.majority_index[i])\n",
    "\n",
    "      rows2 = self.minority_index.copy()\n",
    "\n",
    "      rows = rows1+rows2\n",
    "\n",
    "      train_data=[self.train_x[i,:] for i in rows]\n",
    "      test_data=self.test_x.copy()\n",
    "\n",
    "      # model=clf.fit(train_data,trainy[rows])\n",
    "      abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "      model=abc.fit(train_data,self.train_y[rows])\n",
    "\n",
    "      \n",
    "      if self.method_name==\"f1_score\":\n",
    "        # print(\"here for f1_score\")\n",
    "        predicted_y=model.predict(self.test_x)\n",
    "        f1_0=f1_score(self.test_y,predicted_y,pos_label=0)\n",
    "        f1_1=f1_score(self.test_y,predicted_y,pos_label=1)\n",
    "        # print(f1_0,f1_1)\n",
    "        return min(f1_0,f1_1)\n",
    "      \n",
    "      else:\n",
    "        #check here possible error from here\n",
    "        #changed 1 to -1\n",
    "        false_positive_rate,true_positive_rate,thresholds=roc_curve(self.test_y,model.predict_proba(test_data)[:,-1])\n",
    "        # false_positive_rate,true_positive_rate,thresholds=roc_curve(testy,model.predict_proba(test_data)[:,1])\n",
    "        return auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    def initialize(self,n,select_percent:float):\n",
    "      select_n=round(n*select_percent)\n",
    "      ans=np.zeros(shape=(self.agent_num,n))\n",
    "      for i in range(self.agent_num):\n",
    "        ans[i,random.sample(range(n),k=select_n)]=1\n",
    "      return ans\n",
    "\n",
    "    def get_all_fitness(self,population):\n",
    "      ans=[]\n",
    "      for vec in population:\n",
    "        ans.append(self.obj_func(vec))\n",
    "      ans=np.array(ans)\n",
    "      return ans\n",
    "    \n",
    "    #probably make it a class method but mehh\n",
    "    def sigmoid(self,x:float)->int:\n",
    "      threshold=0.5\n",
    "      if 1/(1+np.e**(-x))>threshold:\n",
    "        return 1\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    def sigmoid_transform(self,population):\n",
    "      new_population=np.zeros_like(population)\n",
    "      for i in range(len(new_population)):\n",
    "        new_population[i]=self.sigmoid(population[i])\n",
    "      return new_population\n",
    "\n",
    "    def thresolding_transform(self,agent,thresold=0.5):\n",
    "      new_agent=[0]*len(agent)\n",
    "      for i in range(len(agent)):\n",
    "        new_agent[i]=int((agent[i]>thresold))\n",
    "      return new_agent\n",
    "\n",
    "    def penalize(self,population,best_fitness_all,penalty,iter_no):\n",
    "      arg_sorted=np.argsort(best_fitness_all)\n",
    "      #keep a 2d array like (iteration_no,one_or_zero_in_worst_solution)\n",
    "      #if you see, some instance present in last 5 iterations, then penalize it.\n",
    "      #continue like this\n",
    "      \n",
    "      consider_no=5\n",
    "      self.worst_cases[iter_no]=population[np.argmin(best_fitness_all)].copy()\n",
    "\n",
    "      # 100\n",
    "      # agent0=>0 1 2 3 4.. 100\n",
    "      #         1 0 1 1 \n",
    "\n",
    "      if iter_no<consider_no:\n",
    "        return\n",
    "      to_penalize=[]\n",
    "      for i in range(np.shape(population)[1]):\n",
    "        for j in range(iter_no-consider_no+1,iter_no+1):\n",
    "          if self.worst_cases[j][i]<=0.5:\n",
    "            break\n",
    "        if j==iter_no:\n",
    "          to_penalize.append(i)\n",
    "      \n",
    "      for i in to_penalize:\n",
    "        # penalty[i]+=1\n",
    "        penalty[i]+=self.p_inc\n",
    "        # print(f\"penalized {i}, cur_penalty:{penalty[i]}\")\n",
    "\n",
    "      # raise NotImplementedError()\n",
    "      \n",
    "    def get_majority_minority_indices(self):\n",
    "      #work with train_data here\n",
    "      type_0=[]\n",
    "      type_1=[]\n",
    "      for i in range(len(self.train_y)):\n",
    "        if self.train_y[i]==0:\n",
    "          type_0.append(i)\n",
    "        else:\n",
    "          type_1.append(i)\n",
    "      if len(type_1)>len(type_0):\n",
    "        self.majority_index,self.minority_index=type_1,type_0\n",
    "      else:\n",
    "        self.majority_index,self.minority_index=type_0,type_1\n",
    "\n",
    "\n",
    "\n",
    "    def optimize(self,select_percent:float=0.5,p_inc:float=1,bounds=None):\n",
    "      #initializing\n",
    "      # n=np.shape(data)[0]\n",
    "      self.p_inc=p_inc\n",
    "\n",
    "      self.get_majority_minority_indices()\n",
    "\n",
    "      n=len(self.majority_index)\n",
    "      if bounds==None:\n",
    "        bounds=[0]*n\n",
    "        for i in range(len(bounds)):\n",
    "          bounds[i]=[0,1]\n",
    "      bounds=np.array(bounds)\n",
    "\n",
    "      population=self.initialize(n,select_percent)\n",
    "      penalty=np.zeros(n)\n",
    "      \n",
    "      best_fitness_all=self.get_all_fitness(population)\n",
    "      best_fitness_all_position=population.copy()\n",
    "      best_fitness_global=np.max(best_fitness_all)\n",
    "      best_fitness_global_position=population[np.argmax(best_fitness_all)].copy()\n",
    "\n",
    "      velocity=np.zeros_like(population)\n",
    "      \n",
    "      penalty_constant=1\n",
    "      # [0,2,0]*[1,1,1]=[0,2,0]=>2\n",
    "      # x=>agent\n",
    "      compound_obj_func=lambda x:self.obj_func(x)-penalty_constant*np.sum(penalty*self.thresolding_transform(x))\n",
    "\n",
    "      # TODO\n",
    "      #apply sigmoid on penalty part of the compound function to reduce dominance of the penalty\n",
    "\n",
    "\n",
    "      #just an initialization\n",
    "      self.worst_cases=[0]*self.max_iter\n",
    "\n",
    "      for iter in range(self.max_iter):\n",
    "        w=0.9-(iter/self.max_iter)*(0.9-0.4)\n",
    "        c1=1.5+np.random.random()*(2-1.5)\n",
    "        c2=2+np.random.random()*(2.5-2)\n",
    "\n",
    "        for i in range(self.agent_num):\n",
    "          temp_velo_term1=w*velocity[i]\n",
    "          # temp_velo_term2=np.multiply(np.random.random_integers(0,1,n),(best_fitness_all_position[i]-population[i]))*c1\n",
    "          temp_velo_term2=np.multiply(self.thresolding_transform(np.random.rand(n)),(best_fitness_all_position[i]-population[i]))*c1\n",
    "          # temp_velo_term3=np.multiply(np.random.random_integers(0,1,n),(best_fitness_global_position-population[i]))*c2\n",
    "          temp_velo_term3=np.multiply(self.thresolding_transform(np.random.rand(n)),(best_fitness_global_position-population[i]))*c2\n",
    "          \n",
    "          temp_velocity=temp_velo_term1+temp_velo_term2+temp_velo_term3\n",
    "          \n",
    "          #updation of velocity that i forgot\n",
    "          velocity[i]=temp_velocity\n",
    "\n",
    "          temp_population=population[i]+temp_velocity\n",
    "\n",
    "          #bringing back into bound\n",
    "          temp_population=np.clip(temp_population,0,1)\n",
    "\n",
    "          #temp_population=self.sigmoid_transform(temp_population)#making the floats 0/1\n",
    "          temp_fitness=compound_obj_func(temp_population)# calling the compound function\n",
    "          if(temp_fitness>best_fitness_all[i]):\n",
    "            population[i]=temp_population.copy()\n",
    "            best_fitness_all[i]=temp_fitness\n",
    "            best_fitness_all_position[i]=temp_population.copy()\n",
    "            if temp_fitness>best_fitness_global:\n",
    "              best_fitness_global=temp_fitness\n",
    "              best_fitness_global_position=temp_population.copy()\n",
    "        self.loggings.append(best_fitness_global)\n",
    "        # print(f\"{iter}:{best_fitness_global}\")\n",
    "        self.penalize(population,best_fitness_all,penalty,iter)\n",
    "\n",
    "      ans_to_return=self.dataset_indices(best_fitness_global_position)\n",
    "      return ans_to_return\n",
    "\n",
    "\n",
    "    def dataset_indices(self,agent):\n",
    "      rows=[]\n",
    "      for i in range(len(agent)):\n",
    "        if agent[i]==1:\n",
    "          rows.append(self.majority_index[i])\n",
    "      rows+=self.minority_index\n",
    "      return rows\n",
    "\n",
    "        \n",
    "  def calc_fitness(train_x,final_test_x,train_y,final_test_y,method_name=\"f1_score\"):\n",
    "\n",
    "    abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "    model=abc.fit(train_x,train_y)\n",
    "    if method_name==\"f1_score\":\n",
    "      # print(\"here for f1_score\")\n",
    "      predicted_y=model.predict(test_x)\n",
    "      f1_0=f1_score(test_y,predicted_y,pos_label=0)\n",
    "      f1_1=f1_score(test_y,predicted_y,pos_label=1)\n",
    "      return min(f1_0,f1_1)\n",
    "      \n",
    "    else:\n",
    "      #check here possible error from here\n",
    "      #changed 1 to -1\n",
    "      false_positive_rate,true_positive_rate,thresholds=roc_curve(final_test_y,model.predict_proba(final_test_x)[:,-1])\n",
    "      # false_positive_rate,true_positive_rate,thresholds=roc_curve(testy,model.predict_proba(test_data)[:,1])\n",
    "      return auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "  #num_agent,max_iter,select_percent,p_inc\n",
    "  all_agent_nums=range(10,40,10)\n",
    "  all_max_iternums=range(10,30,5)\n",
    "  # all_select_percent_s=[0.3,0.5,0.7]\n",
    "  all_select_percent_s=[0.5,0.7]\n",
    "  # all_p_incs=[0.01,0.05,0.1,3]\n",
    "  all_p_incs=[0.01,0.1,0.5]\n",
    "  new_hyperparameter_list=list(it.product(all_agent_nums,all_max_iternums,all_select_percent_s,all_p_incs))\n",
    "  len(new_hyperparameter_list)\n",
    "  new_hyperparameter_list=list(it.product([40],[50],[0.5],[0.01]))\n",
    "  all_results=[]\n",
    "\n",
    "  # method_to_use=\"f1_score\"\n",
    "  method_to_use=\"roc_auc\"\n",
    "  hyper_parameter_list=[\n",
    "  (0.3,0.01),\n",
    "  (0.4,0.01),\n",
    "  (0.4,0.5),\n",
    "  (0.5,1),\n",
    "  (0.5,0.01),\n",
    "  (0.5,0.05),\n",
    "  (0.5,3),\n",
    "  (0.7,1),\n",
    "  (0.7,3),\n",
    "  (0.7,0.01),\n",
    "  (0.7,0.1),\n",
    "  (0.3,1),\n",
    "  (0.5,2),\n",
    "  (0.5,0.5),\n",
    "  (0.7,1),\n",
    "  (0.7,0.05),\n",
    "  (0.4,0.1),\n",
    "  (0.4,1),\n",
    "  (0.5,3),\n",
    "  (0.5,0.001),\n",
    "  ]\n",
    "  hyper_parameter_list=it.product([0.3,0.4,0.5,0.7],[0.001,0.0001,0.01,0.1,0.5,1,3])\n",
    "  agent_num=40\n",
    "  max_iter=25\n",
    "  for select_percent,p_inc in hyper_parameter_list :\n",
    "    train_x,test_x,train_y,test_y=train_test_split(first_train_x_array,first_train_y_array,test_size=0.2,random_state=round(time.time()))\n",
    "    model=PSO(agent_num,max_iter,None,train_x,test_x,train_y,test_y,method_to_use)\n",
    "    rows=model.optimize(select_percent,p_inc)\n",
    "    \n",
    "    total_majority=len(df[df.iloc[:,-1]==max_index])\n",
    "    total_minority=len(df[df.iloc[:,-1]==min_index])\n",
    "    selected_train=len(rows)\n",
    "    total_train=len(train_y)\n",
    "    \n",
    "    new_result=[file_name,total_majority,total_minority,selected_train,total_train,select_percent,p_inc,selected_train/total_train,calc_fitness(train_x[rows],final_test_x,train_y[rows],final_test_y)]\n",
    "\n",
    "    # new_result=f\"{file_name},{agent_num},{max_iter},{select_percent},{p_inc},{calc_fitness(train_x[rows],final_test_x,train_y[rows],final_test_y,method_to_use)}\"\n",
    "    all_results.append(new_result)\n",
    "    # print(all_results)\n",
    "  with open(\"./results/perc_result.csv\",\"a\") as f:\n",
    "    # f.write(f\"file_name: {file_name}\\n\")\n",
    "    # f.writelines(f\"iter_{i+1}: {model.loggings[i]}\"+\"\\n\" for i in range(len(model.loggings)))\n",
    "    \n",
    "    f.writelines(\",\".join(map(str,i))+\"\\n\" for i in all_results)\n",
    "    # f.writelines(i+\"\\n\" for i in all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_results)):\n",
    "  # all_results[i][-2]=all_results[i][-6]/all_results[i][-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./results/perc_result.csv\",\"a\") as f:\n",
    "#   # f.write(f\"file_name: {file_name}\\n\")\n",
    "#   # f.writelines(f\"iter_{i+1}: {model.loggings[i]}\"+\"\\n\" for i in range(len(model.loggings)))\n",
    "\n",
    "#   f.writelines(\",\".join(map(str,i))+\"\\n\" for i in all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result_header=['dataset_name','total_strong_class','strong_class_in_training','strong_class_in_testing','total_rows','selected_rows','select_percent','p_inc','score ']\n",
    "print(*result_header,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result_entry=[file_name,len(df[df.iloc[:,-1]==1]),np.count_nonzero(train_y[rows]),np.count_nonzero(final_test_y),len(train_x),len(rows),select_percent,p_inc,calc_fitness(train_x[rows],final_test_x,train_y[rows],final_test_y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print(*result_entry,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_df=pd.read_csv('continuous_results.csv')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grp=new_df.groupby(\"dataset_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i,j in grp:\n",
    "  print(i,np.max(j.iloc[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comp_table_df=pd.read_csv(\"comp_table.csv\")\n",
    "comp_table_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e95b2ef9755d6679a8ad666f5bec9258488f24667f34995d9697b030437f6467"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
